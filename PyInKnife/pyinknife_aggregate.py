#!/usr/bin/env python
# -*- Mode: python; tab-width: 4; indent-tabs-mode:nil; coding:utf-8 -*-

#    pyinknife_aggregate.py
#
#    Aggregate data generated by ``pyinknife_run``.
#
#    Copyright (C) 2023 Valentina Sora 
#                       <sora.valentina1@gmail.com>
#                       Juan Salamanca Viloria 
#                       <juan.salamanca.viloria@gmail.com>
#                       Matteo Tiberti 
#                       <matteo.tiberti@gmail.com>
#                       Elena Papaleo
#                       <elenap@cancer.dk>
#
#    This program is free software: you can redistribute it and/or
#    modify it under the terms of the GNU General Public License as
#    published by the Free Software Foundation, either version 3 of
#    the License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public
#    License along with this program. 
#    If not, see <http://www.gnu.org/licenses/>.


# Standard library
import argparse
import collections
import logging as log
import os
import os.path
# Third-party packages
import pandas as pd
import yaml
# PyInKnife2
from . import util


def main():


    #------------------------ Argument parser ------------------------#


    # Description of the script
    description = \
        "\nAggregate data generated by the PyinKnife pipeline.\n"
    
    # Create the argument parser
    parser = argparse.ArgumentParser(description = description)
    
    # Add arguments
    c_help = \
        f"The configuration file that was used to run the " \
        f"pipeline. The default is '{util.DEF_CONFIG}'."
    parser.add_argument("-c", "--configfile",
                        type = str,
                        default = util.DEF_CONFIG,
                        help = c_help)

    ca_help = \
        f"The configuration file that will be used for data " \
        f"aggregation. The default is '{util.DEF_CONFIG_AGGR}'."
    parser.add_argument("-ca", "--configfile-aggregate",
                        type = str,
                        default = util.DEF_CONFIG_AGGR,
                        help = ca_help)

    d_help = \
        "The directory where the pipeline was run. Use '.' to " \
        "indicate that the pipeline was run in the current " \
        "working directory."
    parser.add_argument("-d", "--rundir",
                        type = str,
                        required = True,
                        help = d_help)

    od_help = \
        "The directory where the output files will be saved."
    parser.add_argument("-od", "--outdir",
                        type = str,
                        required = True,
                        help = od_help)
    
    firstccs_def = 5 
    firstccs_help = \
        f"The first # most populated connected components " \
        f"to be considered when aggregating the data. " \
        f"The default is {firstccs_def}."
    parser.add_argument("--firstccs",
                        type = int,
                        default = firstccs_def,
                        help = firstccs_help)
    
    # Parse the arguments
    args = parser.parse_args()
    
    # Get the arguments
    config_file = util.get_abspath(args.configfile)
    config_file_aggregate = util.get_abspath(args.configfile_aggregate)
    run_dir = util.get_abspath(args.rundir)
    out_dir = util.get_abspath(args.outdir)
    first_ccs = args.firstccs


    #---------------------------- Logging ----------------------------#


    # Get the module logger
    logger = log.getLogger(__name__)

    # Configure the logger
    log.basicConfig(level = log.INFO)


    #------------------------- Configuration -------------------------#


    # Try to load the configuration used for the run
    try:

        config = \
            util.get_pyinknife_configuration(\
                config_file = config_file)

    # If something went wrong
    except Exception as e:

        # Log an error and exit
        errstr = \
            f"It was not possible to load the configuration used " \
            f"for the run from '{config_file}'. Error: {e}"
        logger.exception(errstr)
        sys.exit(errstr)
    
    # Try to load the configuration for data aggregation
    try:

        aggr_config = \
            yaml.safe_load(open(config_file_aggregate, "r"))

    # If something went wrong
    except Exception as e:

        # Log an error and exit
        errstr = \
            f"It was not possible to load the configuration for " \
            f"data aggregation from '{config_file_aggregate}'. " \
            f"Error: {e}"
        logger.exception(errstr)
        sys.exit(errstr)

    # Get the output file name for the hubs calculation
    out_hubs = config["graph_analysis"]["hubs"]["out"]
    
    # Get the output file name for the connected components
    # calculation
    out_cc = config["graph_analysis"]["ccs"]["out"]
    
    # Get the name of the directory where the results for the full
    # trajectory are stored 
    trj_dir_name = config["resampling"]["dirnames"]["trj"]

    # Get the format specifications for the output dataframes
    extension = aggr_config["output"]["extension"]
    sep = aggr_config["output"]["sep"]

    # Get the column names for the statistics to be computed
    mean_col = aggr_config["columns"]["mean"]
    median_col = aggr_config["columns"]["median"]
    min_col = aggr_config["columns"]["min"]
    max_col = aggr_config["columns"]["max"]
    std_col = aggr_config["columns"]["std"]
    sem_col = aggr_config["columns"]["sem"]


    #----------------------- Running directory -----------------------#


    # Create a defaultdict defaulting to a Pandas data frame
    # to store the results
    results = collections.defaultdict(pd.DataFrame)
    
    # Get the running directory base name so that the paths reported
    # by os.walk() will be rooted there
    run_dir_basename = os.path.basename(run_dir)

    # If the base name of the running directory is the same as the
    # base name of the current working directory (i.e., pyinknife_run
    # was run in the current working directory)
    if run_dir_basename == os.path.basename(os.getcwd()):

        # The base name will be '.' (current directory)
        run_dir_basename = "."


    #------------------------- Data parsing --------------------------#

    
    # Traverse the directory tree rooted in the running directory
    for path, dirs, files in list(os.walk(run_dir_basename)):

        # Split the path into its components
        split_path = path.split("/")
        
        # If the path is not a leaf (there are sub-directories)
        if dirs:
            
            # Keep traversing (the results are stored in the leaf
            # paths)
            continue
        
        # If the path is 5-directories deep
        if len(split_path) == 6:
            
            # The analysis had only one possible mode
            trj, analysis, d_cut, p_cut, graph_analysis = \
                *split_path[1:4], *split_path[-2:]
            
            # Build the key that will identify the results for
            # this analysis (name of the analysis, distance
            # cut-off, occurrence cut-off, and name of the
            # graph analyis)
            key = f"{analysis}_{d_cut}_{p_cut}_{graph_analysis}"
        
        # if the path is 6-directories deep
        elif len(split_path) == 7:

            # The analysis had multiple modes
            trj, analysis, mode, d_cut, p_cut, graph_analysis = \
                *split_path[1:5], *split_path[-2:]
            
            # Build the key that will identify the results for
            # this analysis
            key = f"{analysis}_{mode}_{d_cut}_{p_cut}_{graph_analysis}"

        # Otherwise
        else:

            # Ignore the path since it does not contain the
            # data we are interested in
            continue
        
        # If this leaf corresponds to the results for hubs 
        if graph_analysis == "hubs":
            
            # Get the path to the output file
            out_file = os.path.join(path, out_hubs)

            # Parse the output file as a Pandas Series
            result = util.parse_hubs_out(out_file = out_file)
        
        # If this leaf corresponds to the results for connected
        # components
        elif graph_analysis == "ccs":
            
            # Get the output file path
            out_file = os.path.join(path, out_cc)
            
            # Parse the output file as a Pandas Series
            result = util.parse_cc_out(out_file = out_file,
                                       first_ccs = first_ccs)
        
        # The name of the Series will be the name of the trajectory
        result.name = trj
        
        # Add the result for the current trajectory as a new column
        # in the data frame storing data for this analysis
        results[key][trj] = result


    #----------------------- Data aggregation ------------------------#


    # If the output directory does not exist yet
    if not os.path.exists(out_dir):

        # Create it
        os.makedirs(out_dir, exist_ok = True)

        # Inform the user that the directory was created
        infostr = \
            f"The directory '{out_dir}' was created."
        logger.info(infostr)

    # Otherwise
    else:

        # Inform the user that the directory was found
        infostr = \
            f"The directory '{out_dir}' was found."
        logger.info(infostr)

    # For each output name and corresponding data frame
    for out_name, df in results.items():

        # Set the path to the output file that will contain the
        # aggregated data
        out_aggr_file = os.path.join(out_dir, out_name + extension)
        
        # Select only those columns containing results from
        # resamplings to compute the statistics
        df_resamplings = df.loc[:, df.columns != trj_dir_name]
        
        # Put the column identifying the full trajectory first
        df_full_trj = df[trj_dir_name]
        df = df.drop(labels = [trj_dir_name],
                     axis = 1)
        df.insert(0, trj_dir_name, df_full_trj)
        
        # Compute the mean over the resamplings and add the
        # result as a column
        df[mean_col] = df_resamplings.mean(axis = 1)

        # Compute the median over the resamplings and add the
        # result as a column
        df[median_col] = df_resamplings.median(axis = 1)

        # Compute the minimum over the resamplings and add the
        # result as a column
        df[min_col] = df_resamplings.min(axis = 1)

        # Compute the maximum over the resamplings and add the
        # result as a column
        df[max_col] = df_resamplings.max(axis = 1)

        # Compute the standard deviation over the resamplings
        # and add the result as a column
        df[std_col] = df_resamplings.std(axis = 1)

        # Compute the standard error of the mean over the
        # resamplings and add the result as a column
        df[sem_col] = df_resamplings.sem(axis = 1)

        # Sort the index so that more populated connected
        # components and hubs with lower degrees go first
        df = df.sort_index()
        
        # Write the output file
        df.to_csv(out_aggr_file,
                  sep = sep)


if __name__ == "__main__":
    main()